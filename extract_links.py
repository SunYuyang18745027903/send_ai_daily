#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æå–æŒ‡å®šç½‘ç«™ä¸­çš„åŸæ–‡é“¾æ¥ä¸­çš„è¶…é“¾æ¥
"""

import os
import sys
import logging
import requests
from bs4 import BeautifulSoup

# è®¾ç½® UTF-8 è¾“å‡ºï¼Œé¿å… Windows ä¸‹ GBK ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å›ºå®šåŸºç¡€ URL
BASE_URL = "http://100.68.66.102:18001/views/article/"

def extract_links_from_url(url):
    """
    ä»æŒ‡å®š URL æå–è¶…é“¾æ¥
    """
    try:
        logger.info(f"è®¿é—® URL: {url}")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # è§£æ HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ‰€æœ‰è¶…é“¾æ¥
        links = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag.get('href')
            text = a_tag.get_text(strip=True)
            links.append({
                'href': href,
                'text': text
            })
        
        logger.info(f"ä» {url} æå–åˆ° {len(links)} ä¸ªè¶…é“¾æ¥")
        return links
    except Exception as e:
        logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
        return []


def find_original_link(url):
    """
    ä»æŒ‡å®š URL ä¸­æŸ¥æ‰¾åŸæ–‡é“¾æ¥
    """
    try:
        logger.info(f"æŸ¥æ‰¾åŸæ–‡é“¾æ¥: {url}")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        # è§£æ HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ç²¾ç¡®å®šä½åŒ…å«"ğŸ”— åŸæ–‡é“¾æ¥"æ–‡æœ¬çš„aæ ‡ç­¾
        original_link = None
        for a_tag in soup.find_all('a', href=True):
            text = a_tag.get_text(strip=True)
            if 'ğŸ”— åŸæ–‡é“¾æ¥' == text:
                href = a_tag.get('href')
                # å¤„ç†hrefä¸­å¯èƒ½çš„åå¼•å·åŒ…å›´
                if href.startswith('`') and href.endswith('`'):
                    href = href.strip('`')
                original_link = href
                break
        
        if original_link:
            logger.info(f"æ‰¾åˆ°åŸæ–‡é“¾æ¥: {original_link}")
            return original_link
        else:
            logger.warning("æœªæ‰¾åˆ°åŸæ–‡é“¾æ¥")
            return None
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾åŸæ–‡é“¾æ¥å¤±è´¥: {e}")
        return None


def get_original_link(input_str):
    """
    æ ¹æ®è¾“å…¥å­—ç¬¦ä¸²æŠ“å–åŸæ–‡é“¾æ¥
    
    Args:
        input_str (str): è¾“å…¥å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ "3271041950-2652670441_2" æˆ– "rss/feed/3271041950-2652670441_2"
    
    Returns:
        str: æŠ“å–åˆ°çš„åŸæ–‡é“¾æ¥ï¼Œä¾‹å¦‚ "https://mp.weixin.qq.com/s/N1PQuc2P1ycI575EiNmuyg"ï¼Œ
             å¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    # æˆªå–å­—ç¬¦ä¸²ï¼Œè·å–æ–‡ç«  ID
    # ä¾‹å¦‚ä» "rss/feed/3271041950-2652670441_2" ä¸­æå– "3271041950-2652670441_2"
    article_id = input_str.split('/')[-1]
    logger.info(f"å¼€å§‹å¤„ç†è¾“å…¥å­—ç¬¦ä¸²: {input_str}")
    logger.info(f"æå–åˆ°çš„æ–‡ç«  ID: {article_id}")
    
    # æ‹¼æ¥å®Œæ•´ URL
    url = BASE_URL + article_id
    
    # æŸ¥æ‰¾åŸæ–‡é“¾æ¥
    original_link = find_original_link(url)
    
    if original_link:
        logger.info(f"æˆåŠŸæŠ“å–åˆ°åŸæ–‡é“¾æ¥: {original_link}")
        return original_link
    else:
        logger.warning(f"æœªæ‰¾åˆ°åŸæ–‡é“¾æ¥: {url}")
        return ""


if __name__ == "__main__":
    # å½“ç›´æ¥è¿è¡Œæ­¤è„šæœ¬æ—¶ï¼Œæ‰§è¡Œç®€å•çš„å¯¼å…¥æ£€æŸ¥
    logger.info("extract_links.py æ¨¡å—åŠ è½½æˆåŠŸï¼Œå¯é€šè¿‡ import extract_links ä½¿ç”¨å…¶ä¸­çš„å‡½æ•°")
    logger.info("ä¸»è¦å‡½æ•°: get_original_link(input_str)")
    logger.info("ç¤ºä¾‹ç”¨æ³•: extract_links.get_original_link('rss/feed/3271041950-2652670441_2')")
