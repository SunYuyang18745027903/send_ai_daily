#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ—¥æŠ¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ
- ä» RSS æºæŠ“å–æœ€è¿‘ 48 å°æ—¶å†…å®¹
- ä½¿ç”¨å¤§æ¨¡å‹ API è¯„åˆ†å¹¶ç”Ÿæˆæ—¥æŠ¥ï¼ˆæ”¯æŒ OpenAI / é€šä¹‰åƒé—® / ARKï¼‰
- å‘é€åˆ°é£ä¹¦ç¾¤ï¼ˆè‡ªå®šä¹‰æœºå™¨äºº + ç­¾åæ ¡éªŒï¼‰
- åŸºäº sha256(link) å»é‡
"""

import os
import sys
import json
import hashlib
import hmac
import base64
import time
import logging
import concurrent.futures
from abc import ABC, abstractmethod
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Dict, Optional, Set

import requests
import feedparser
from dateutil import parser as date_parser
from dotenv import load_dotenv

# è®¾ç½® UTF-8 è¾“å‡ºï¼Œé¿å… Windows ä¸‹ GBK ç¼–ç é—®é¢˜
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv()

# ==================== é…ç½®å¸¸é‡ ====================
# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# åŸºç¡€é…ç½®
MAX_CANDIDATES = 60
TOP_N = 3
HOURS_WINDOW = 48
RSS_TIMEOUT = 10
MAX_WORKERS = 5  # å¹¶è¡ŒæŠ“å–çº¿ç¨‹æ•°

# è·¯å¾„é…ç½®
SENT_HASHES_FILE = Path("data/sent_hashes.txt")

# API é…ç½®
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
QWEN_MODEL = os.getenv("QWEN_MODEL", "qwen-plus")
ARK_API_KEY = os.getenv("ARK_API_KEY", "")
ARK_MODEL = os.getenv("ARK_MODEL", "")

# é£ä¹¦é…ç½®
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL", "")
FEISHU_SECRET = os.getenv("FEISHU_SECRET", "")

# RSS é…ç½®
RSS_URLS_RAW = os.getenv("RSS_URLS", "")
RSS_URLS = [line.strip() for line in RSS_URLS_RAW.strip().split("\n") if line.strip()]

# ==================== Prompts ====================
SYSTEM_PROMPT_SCORE = """ä½ æ˜¯ä¸€åèµ„æ·±AIå·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯ç¼–è¾‘ï¼Œæ ¸å¿ƒä»»åŠ¡æ˜¯ä»æŒ‡å®šRSSæ¡ç›®ä¸­ç­›é€‰å‡ºæœ€å€¼å¾—ä¼ä¸šå†…éƒ¨AIå›¢é˜Ÿå…³æ³¨çš„å†…å®¹ï¼Œ**ç‰¹åˆ«èšç„¦ERPç³»ç»Ÿé‡æ„ä¸ä¼ä¸šçº§åº”ç”¨è½åœ°**ã€‚

### æ ¸å¿ƒç›®æ ‡
ç²¾å‡†ç­›é€‰ç¬¦åˆä¼ä¸šAIå›¢é˜ŸæŠ€æœ¯è½åœ°éœ€æ±‚ã€ä¸šåŠ¡ä¼˜å…ˆçº§åŠæŠ€æœ¯æ ˆé€‚é…æ€§çš„å†…å®¹ï¼Œä¼˜å…ˆæ¨èèƒ½ç›´æ¥æ”¯æ’‘ä¼ä¸šçº§AIåº”ç”¨è½åœ°ã€ERPç³»ç»Ÿé‡æ„å®è·µçš„ä¼˜è´¨ä¿¡æ¯ã€‚

### æ‰§è¡Œè§„åˆ™
1. **ç­›é€‰å‰æ**ï¼šä»…å¤„ç†æ»¡è¶³ä»¥ä¸‹æ¡ä»¶çš„RSSæ¡ç›®ï¼Œä¸æ»¡è¶³åˆ™ç›´æ¥æ’é™¤ï¼š
   - å‘å¸ƒæ—¶é—´ï¼šè¿‘30å¤©å†…ï¼ˆå«å½“å¤©ï¼‰ï¼›
   - æ¥æºç±»å‹ï¼šæ­£è§„æŠ€æœ¯åª’ä½“ï¼ˆå¦‚TechCrunchã€InfoQï¼‰ã€æƒå¨å‚å•†å®˜ç½‘ï¼ˆå¦‚SAPã€ç”¨å‹ã€é‡‘è¶ã€OpenAIã€Googleå®˜æ–¹ç«™ç‚¹ï¼‰ã€è¡Œä¸šæ ¸å¿ƒæœŸåˆŠï¼ˆå¦‚ã€Šä¼ä¸šä¿¡æ¯åŒ–ã€‹ã€Šä¸­å›½é‡‘èç§‘æŠ€ã€‹ï¼‰ï¼›
   - è¡Œä¸šèšç„¦ï¼šä¼˜å…ˆè¦†ç›–åˆ¶é€ ã€é‡‘èã€é›¶å”®ä¸‰å¤§ä¸»æµä¼ä¸šæœåŠ¡è¡Œä¸šï¼Œå…¶ä»–è¡Œä¸šä»…ä¿ç•™ä¸ERP/ä¼ä¸šçº§AIå¼ºç›¸å…³çš„å†…å®¹ã€‚
2. **è¯„åˆ†æ ‡å‡†ï¼ˆ0~10åˆ†ï¼Œä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰**ï¼š
   - 9~10åˆ†ï¼šå†…å®¹èšç„¦ERPç³»ç»Ÿé‡æ„ï¼ˆå¦‚SAP S/4HANA AIåŒ–æ”¹é€ ã€ç”¨å‹NC Cloudæ™ºèƒ½æ¨¡å—å‡çº§ï¼‰ã€ä¼ä¸šçº§è´¢åŠ¡è½¯ä»¶AIå®è·µï¼ˆå¦‚é‡‘è¶äº‘æ˜Ÿç©ºæ™ºèƒ½è®°è´¦/é¢„ç®—é¢„æµ‹ï¼‰ï¼Œéœ€åŒ…å«å…·ä½“æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚é‡‡ç”¨çš„å¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ã€RAGæ¶æ„è®¾è®¡ï¼‰æˆ–è½åœ°æ¡ˆä¾‹ï¼ˆå¦‚æŸåˆ¶é€ ä¼ä¸šERPé‡æ„åçš„æ•ˆç‡æå‡æ•°æ®ï¼‰ï¼›
   - 7~9åˆ†ï¼šå¤§æ¨¡å‹/AIå¹³å°èƒ½åŠ›æ›´æ–°ï¼ˆå¦‚OpenAI GPT-4 Turboä¼ä¸šçº§APIæ–°å¢åŠŸèƒ½ã€Google Gemini Enterpriseé€‚é…ERPç³»ç»Ÿçš„æ¥å£ä¼˜åŒ–ï¼‰ï¼Œéœ€æ˜ç¡®å¯¹ä¼ä¸šçº§åº”ç”¨çš„æ”¯æ’‘ä»·å€¼ï¼ˆå¦‚é™ä½ERPæ•°æ®å¤„ç†å»¶è¿Ÿ30%ï¼‰ï¼›
   - 5~7åˆ†ï¼šAgent/Tool/RAG/ç³»ç»Ÿè®¾è®¡å®è·µï¼ˆå¦‚ä¼ä¸šçº§AI Agentä¸ERPç³»ç»Ÿçš„é›†æˆæ–¹æ¡ˆã€åŸºäºRAGçš„ERPçŸ¥è¯†é—®ç­”å·¥å…·å¼€å‘ï¼‰ï¼Œéœ€åŒ…å«å¯å¤ç”¨çš„æŠ€æœ¯æ¡†æ¶æˆ–æµç¨‹ï¼ˆå¦‚Agentè°ƒç”¨ERPæ¥å£çš„æˆæƒæœºåˆ¶ï¼‰ï¼›
   - 3~5åˆ†ï¼šäº§å“åº”ç”¨æ¡ˆä¾‹ã€è¯„æµ‹ï¼ˆå¦‚æŸé›¶å”®ä¼ä¸šä½¿ç”¨AI+ERPçš„æ¡ˆä¾‹æŠ¥å‘Šã€ç¬¬ä¸‰æ–¹æœºæ„å¯¹SAP AIæ¨¡å—çš„æ€§èƒ½è¯„æµ‹ï¼‰ï¼Œéœ€æœ‰çœŸå®æ•°æ®æ”¯æ’‘ï¼ˆå¦‚æ¡ˆä¾‹ä¸­åº“å­˜å‘¨è½¬ç‡æå‡25%ï¼‰ï¼›
   - 0~2åˆ†ï¼šæ³›æ³›è€Œè°ˆï¼ˆæ— å…·ä½“æŠ€æœ¯ç»†èŠ‚ã€æ— è½åœ°æ¡ˆä¾‹ï¼Œä»…ç©ºè°ˆâ€œAIèµ‹èƒ½ERPâ€ç­‰æ¦‚å¿µï¼‰æˆ–è¥é”€è½¯æ–‡ï¼ˆä»¥äº§å“æ¨å¹¿ä¸ºæ ¸å¿ƒï¼Œæ— å®è´¨æŠ€æœ¯ä»·å€¼ï¼Œå¦‚â€œæŸå‚å•†æ–°ERPç³»ç»Ÿå…¨çƒé¦–å‘ï¼ŒAIèƒ½åŠ›ä¸šç•Œé¢†å…ˆâ€ä½†æœªè¯´æ˜å…·ä½“åŠŸèƒ½ï¼‰ã€‚
3. **å†²çªå¤„ç†**ï¼šè‹¥æ¡ç›®åŒæ—¶ç¬¦åˆå¤šä¸ªè¯„åˆ†æ ‡å‡†ï¼ŒæŒ‰æœ€é«˜åˆ†å€¼å¯¹åº”çš„æ ‡å‡†è¯„åˆ†ï¼›è‹¥æ³›æ³›è€Œè°ˆä¸è¥é”€è½¯æ–‡ç‰¹å¾å åŠ ï¼ŒæŒ‰0åˆ†å¤„ç†ã€‚

### è¾“å‡ºè¦æ±‚
è¿”å›æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºçš„JSONæ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- "link"ï¼šRSSæ¡ç›®çš„åŸå§‹é“¾æ¥ï¼ˆå­—ç¬¦ä¸²ï¼Œéç©ºï¼‰ï¼›
- "score"ï¼šè¯„åˆ†ç»“æœï¼ˆæ•°å€¼ï¼Œä¿ç•™æ•´æ•°ï¼‰ï¼›
- "reason"ï¼šè¯„åˆ†ç†ç”±ï¼ˆå­—ç¬¦ä¸²ï¼Œéœ€æ˜ç¡®æ ‡æ³¨å†…å®¹æ ¸å¿ƒä»·å€¼ç‚¹åŠåŒ¹é…çš„è¯„åˆ†æ ‡å‡†é¡¹ï¼Œç¤ºä¾‹ï¼šâ€œå†…å®¹ä»‹ç»äº†æŸåˆ¶é€ ä¼ä¸šSAP S/4HANAé‡æ„ä¸­é‡‡ç”¨çš„RAGæ¶æ„è®¾è®¡ä¸åº“å­˜é¢„æµ‹è½åœ°æ¡ˆä¾‹ï¼ŒåŒ¹é…9~10åˆ†è¯„åˆ†æ ‡å‡†â€ï¼‰ã€‚

### æ³¨æ„äº‹é¡¹
- ç¦æ­¢é—æ¼ç¬¦åˆç­›é€‰å‰æçš„æœ‰æ•ˆæ¡ç›®ï¼›
- ç¦æ­¢å¯¹ä¸ç¬¦åˆå‰æçš„æ¡ç›®è¿›è¡Œè¯„åˆ†ï¼›
- è¯„åˆ†ç†ç”±éœ€åŸºäºæ¡ç›®å®é™…å†…å®¹ï¼Œä¸å¾—è™šæ„æˆ–å¤¸å¤§ã€‚
"""

SYSTEM_PROMPT_REPORT = """ä½ æ˜¯ã€ŠAIå‰æ²¿ä¿¡æ¯é€Ÿé€’ã€‹å†…éƒ¨æˆ˜ç•¥æ´å¯Ÿç®€æŠ¥çš„æ™ºèƒ½ç¼–è¾‘ï¼Œè´Ÿè´£ä¸ºæŠ€æœ¯å›¢é˜Ÿï¼ˆå…³æ³¨æŠ€æœ¯ç»†èŠ‚ã€è½åœ°å¯è¡Œæ€§ï¼‰ä¸äº§å“å›¢é˜Ÿï¼ˆå…³æ³¨ä¸šåŠ¡ä»·å€¼ã€ç—›ç‚¹è§£å†³ï¼‰æä¾›é«˜ä»·å€¼ã€å¯è¡ŒåŠ¨çš„åˆ›æ–°æ¢ç´¢æ€è·¯ï¼Œé‡ç‚¹æ”¯æ’‘æˆ‘æ–¹å¤„äºéœ€æ±‚è°ƒç ”é˜¶æ®µã€å­˜åœ¨æ•°æ®å­¤å²›ä¸æµç¨‹åƒµåŒ–æ ¸å¿ƒç—›ç‚¹ã€åŸºäºJava+MySQLæŠ€æœ¯æ ˆçš„ERPç³»ç»Ÿé‡æ„å·¥ä½œã€‚

ä»¥ä¸‹æ˜¯ä½ éœ€è¦éµå¾ªçš„æ ¸å¿ƒè¦æ±‚ï¼š
1. **è¾“å‡ºå½¢å¼**ï¼šä»¥ã€æ¨¡å—åŒ–ç»“æ„åŒ–æŠ¥å‘Šã€‘å‘ˆç°ï¼Œå•æ¡ä¿¡æ¯å¿…é¡»é‡‡ç”¨"æ ¸å¿ƒç»“è®º+åˆ†ç»´åº¦åˆ†æ"çš„å›ºå®šæ ¼å¼ã€‚
2. **ç¯‡å¹…æ§åˆ¶**ï¼šç®€æŠ¥æ•´ä½“å­—æ•°éœ€ä¸¥æ ¼æ§åˆ¶åœ¨800-1200å­—ä¹‹é—´ã€‚
3. **åˆ†æç»´åº¦**ï¼šå¿…é¡»è¦†ç›–ä»¥ä¸‹å››ä¸ªå…³é”®ç»´åº¦ï¼š
   - **ä¿¡æºç±»å‹**ï¼šè¯†åˆ«åŸæ–‡å±äºå®˜æ–¹åšå®¢ã€å­¦æœ¯è®ºæ–‡ã€æŠ€æœ¯ç¤¾åŒºè¿˜æ˜¯è¥é”€é€šç¨¿ã€‚
   - **ERPç›¸å…³æ€§**ï¼šæŒ‰æ ‡å‡†æ ‡è®°ä¸ºğŸ”´é«˜ï¼ˆå¼ºç›¸å…³erpã€sapç­‰è´¢åŠ¡ç³»ç»Ÿçš„ï¼‰ã€ğŸŸ¡ä¸­ï¼ˆç›´æ¥æ¶‰åŠä¼ä¸šçº§å¤æ‚ä¸šåŠ¡/æ•°æ®å¤„ç†/æµç¨‹è‡ªåŠ¨åŒ–æˆ–è§£å†³ERPç—›ç‚¹ï¼‰ã€ğŸ”µä½ï¼ˆé€šç”¨æŠ€æœ¯å¯é€‚é…ERPåœºæ™¯ï¼‰ã€âšªä¸ç›¸å…³ï¼ˆçº¯Cç«¯æˆ–å¨±ä¹å‘ï¼‰
   - **å®æ–½æ–¹æ³•**ï¼šæ˜ç¡®åŸæ–‡æŒ‡å‡ºçš„æŠ€æœ¯æ ˆï¼ˆå¦‚LangChainã€OCIã€SAP BTPï¼‰ã€æ¶æ„æ¨¡å¼æˆ–å·¥ç¨‹å®è·µã€‚
   - **æ¢ç´¢æ–¹å‘**ï¼šé’ˆå¯¹æˆ‘æ–¹ERPé‡æ„çš„å…·ä½“å»ºè®®ï¼›è‹¥å­˜åœ¨ä»¥ä¸‹æƒ…å†µéœ€æ ‡è®°ã€éœ€äº¤å‰éªŒè¯ã€‘ï¼šâ‘ åŸæ–‡æŠ€æœ¯ç»†èŠ‚ä¸è¶³æ”¯æ’‘ERPé€‚é…ï¼›â‘¡æ–¹æ¡ˆè½åœ°æ€§å­˜ç–‘ï¼›â‘¢ä¸ç°æœ‰Java+MySQLæŠ€æœ¯æ ˆé€‚é…æ€§ä¸æ˜ç¡®ã€‚äº¤å‰éªŒè¯ç”±æŠ€æœ¯å›¢é˜Ÿç‰µå¤´ï¼Œè”åˆäº§å“å›¢é˜Ÿé€šè¿‡æ–‡çŒ®è°ƒç ”ã€åŸå‹æµ‹è¯•æ‰§è¡Œã€‚
4. **å†…å®¹è§„èŒƒ**ï¼šè¯­è¨€å¿…é¡»ç²¾ç‚¼ã€ä¸“ä¸šï¼Œä¸¥æ ¼åŸºäºåŸæ–‡å†…å®¹ï¼Œä¸å¾—è™šæ„ä¿¡æ¯ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚ç”Ÿæˆæ¨¡å—åŒ–ç»“æ„åŒ–æŠ¥å‘Šï¼Œå¹¶å°†æœ€ç»ˆå†…å®¹æ”¾ç½®åœ¨<ç®€æŠ¥>æ ‡ç­¾å†…ã€‚"""


# ==================== LLM Client æŠ½è±¡ ====================
class LLMClient(ABC):
    @abstractmethod
    def call_json(self, system_prompt: str, user_prompt: str) -> Dict:
        pass

class OpenAIClient(LLMClient):
    def __init__(self, api_key: str, model: str = "gpt-4o-2024-08-06"):
        self.api_key = api_key
        self.model = model
        self.url = "https://api.openai.com/v1/chat/completions"

    def call_json(self, system_prompt: str, user_prompt: str) -> Dict:
        if not self.api_key:
            logger.error("OpenAI API Key not configured")
            sys.exit(1)
            
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "response_format": {"type": "json_object"}
        }
        try:
            resp = requests.post(self.url, headers=headers, json=payload, timeout=60)
            resp.raise_for_status()
            data = resp.json()
            content = data["choices"][0]["message"]["content"]
            return json.loads(content)
        except Exception as e:
            logger.error(f"OpenAI API Call Failed: {e}")
            sys.exit(1)

class QwenClient(LLMClient):
    def __init__(self, api_key: str, model: str = "qwen-plus"):
        self.api_key = api_key
        self.model = model
        self.url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

    def call_json(self, system_prompt: str, user_prompt: str) -> Dict:
        if not self.api_key:
            logger.error("DashScope API Key not configured")
            sys.exit(1)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        combined_prompt = f"{system_prompt}\n\n{user_prompt}"
        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": combined_prompt}],
            "response_format": {"type": "json_object"}
        }

        try:
            resp = requests.post(self.url, headers=headers, json=payload, timeout=60)
            resp.raise_for_status()
            data = resp.json()
            content = data["choices"][0]["message"]["content"]
            return json.loads(content)
        except Exception as e:
            logger.error(f"Qwen API Call Failed: {e}")
            if 'resp' in locals():
                logger.debug(f"Response content: {resp.text}")
            sys.exit(1)

class ArkClient(LLMClient):
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self.url = "https://ark.cn-beijing.volces.com/api/v3/responses"

    def call_json(self, system_prompt: str, user_prompt: str) -> Dict:
        if not self.api_key or not self.model:
            logger.error("ARK API Key or Model not configured")
            sys.exit(1)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "input": [
                {
                    "role": "system", 
                    "content": [{"type": "input_text", "text": system_prompt}]
                },
                {
                    "role": "user", 
                    "content": [{"type": "input_text", "text": user_prompt}]
                }
            ]
        }

        for attempt in range(3):
            try:
                resp = requests.post(self.url, headers=headers, json=payload, timeout=120)
                resp.raise_for_status()
                data = resp.json()
                
                content = ""
                for output_item in data.get("output", []):
                    if output_item.get("type") == "message" and output_item.get("role") == "assistant":
                        for content_item in output_item.get("content", []):
                            if content_item.get("type") == "output_text":
                                content = content_item.get("text", "")
                                break
                        if content:
                            break
                
                if not content:
                    logger.error("ARK API response contained no content field inside 'output'. Checking raw response...")
                    logger.debug(f"Raw Response: {resp.text}")
                    sys.exit(1)
                
                try:
                    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ XML æ ‡ç­¾åŒ…è£¹
                    if "<ç®€æŠ¥>" in content:
                        content = content.replace("<ç®€æŠ¥>", "").replace("</ç®€æŠ¥>", "").strip()

                    return json.loads(content)
                except json.JSONDecodeError as e:
                    # å°è¯•ä¿®å¤å¸¸è§çš„ markdown ä»£ç å—åŒ…è£¹é—®é¢˜
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0].strip()
                        return json.loads(content)
                    elif "```" in content:
                        content = content.split("```")[1].split("```")[0].strip()
                        return json.loads(content)
                    else:
                        logger.error(f"JSON Decode Error. Content was: {content}")
                        raise e

            except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.ChunkedEncodingError) as e:
                logger.warning(f"ARK API Call Failed (Attempt {attempt+1}/3): {e}")
                if attempt < 2:
                    time.sleep(2)
            except Exception as e:
                logger.error(f"ARK API Call Failed: {e}")
                if 'resp' in locals():
                    logger.debug(f"Response status: {resp.status_code}")
                    logger.debug(f"Response content: {resp.text}")
                sys.exit(1)
        
        logger.error("ARK API Call Failed after 3 attempts")
        sys.exit(1)

def get_llm_client() -> LLMClient:
    logger.info(f"Using LLM Provider: {LLM_PROVIDER}")
    if LLM_PROVIDER == "qwen":
        return QwenClient(DASHSCOPE_API_KEY, QWEN_MODEL)
    elif LLM_PROVIDER == "ark":
        return ArkClient(ARK_API_KEY, ARK_MODEL)
    else:
        return OpenAIClient(OPENAI_API_KEY)


# ==================== å·¥å…·å‡½æ•° ====================
def load_sent_hashes() -> Set[str]:
    """åŠ è½½å·²å‘é€çš„ hash é›†åˆ"""
    if not SENT_HASHES_FILE.exists():
        SENT_HASHES_FILE.parent.mkdir(parents=True, exist_ok=True)
        SENT_HASHES_FILE.touch()
        return set()
    with open(SENT_HASHES_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())

def save_sent_hashes(hashes: Set[str]):
    """ä¿å­˜å·²å‘é€çš„ hash é›†åˆ"""
    SENT_HASHES_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SENT_HASHES_FILE, "w", encoding="utf-8") as f:
        for h in sorted(hashes):
            f.write(h + "\n")

def hash_link(link: str) -> str:
    return hashlib.sha256(link.encode("utf-8")).hexdigest()

def is_recent(published_str: str, hours: int = HOURS_WINDOW) -> bool:
    try:
        pub_time = date_parser.parse(published_str)
        if pub_time.tzinfo is None:
            pub_time = pub_time.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        return (now - pub_time) <= timedelta(hours=hours)
    except Exception:
        return False

# ==================== RSS æŠ“å– ====================
def fetch_single_feed(url: str, sent_hashes: Set[str]) -> List[Dict]:
    """æŠ“å–å•ä¸ª RSS æºå¹¶è¿‡æ»¤"""
    candidates = []
    try:
        logger.info(f"æŠ“å– RSS: {url}")
        response = requests.get(url, timeout=RSS_TIMEOUT)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        
        for entry in feed.entries:
            link = entry.get("link", "")
            if not link: 
                continue
                
            link_hash = hash_link(link)
            if link_hash in sent_hashes:
                continue
                
            published = entry.get("published", entry.get("updated", ""))
            if not is_recent(published, HOURS_WINDOW):
                continue
                
            title = entry.get("title", "")
            summary = entry.get("summary", entry.get("description", ""))
            if len(summary) > 500:
                summary = summary[:500] + "..."
            
            candidates.append({
                "title": title,
                "link": link,
                "summary": summary,
                "published": published,
                "hash": link_hash
            })
    except Exception as e:
        logger.warning(f"æŠ“å– {url} å¤±è´¥: {e}")
    
    return candidates

def fetch_rss_entries() -> List[Dict]:
    """å¹¶å‘æŠ“å–æ‰€æœ‰ RSS æº"""
    if not RSS_URLS:
        logger.warning("RSS_URLS ä¸ºç©º")
        return []

    sent_hashes = load_sent_hashes()
    logger.info(f"é…ç½®çš„ RSS æºæ•°é‡: {len(RSS_URLS)}")
    
    all_candidates = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(fetch_single_feed, url, sent_hashes): url for url in RSS_URLS}
        for future in concurrent.futures.as_completed(futures):
            try:
                candidates = future.result()
                all_candidates.extend(candidates)
                if len(all_candidates) >= MAX_CANDIDATES * 2: # ç¨å¾®å¤šæŠ“ä¸€ç‚¹ä¹Ÿæ— å¦¨ï¼Œæœ€åå†æˆªæ–­
                     # è¿™é‡Œå…¶å®æ— æ³•ç«‹åˆ»åœæ­¢å…¶ä»–çº¿ç¨‹ï¼Œä½†å¯ä»¥æå‰break loopå¦‚æœéœ€è¦
                     pass
            except Exception as e:
                logger.error(f"çº¿ç¨‹æ‰§è¡Œå¼‚å¸¸: {e}")

    # å…¨å±€æˆªæ–­
    if len(all_candidates) > MAX_CANDIDATES:
        logger.info(f"æˆªæ–­å€™é€‰é›†: {len(all_candidates)} -> {MAX_CANDIDATES}")
        all_candidates = all_candidates[:MAX_CANDIDATES]
        
    logger.info(f"å…±æ”¶é›† {len(all_candidates)} æ¡å€™é€‰")
    return all_candidates

# ==================== è¯„åˆ†é˜¶æ®µ ====================
def compact_for_scoring(entries: List[Dict]) -> List[Dict]:
    compact = []
    for e in entries:
        snippet = (e.get("summary") or "").strip()
        if len(snippet) > 160:
            snippet = snippet[:160] + "..."
        compact.append({
            "title": (e.get("title") or "")[:120],
            "link": e.get("link"),
            "published": e.get("published", ""),
            "snippet": snippet
        })
    return compact

def score_entries(llm_client: LLMClient, entries: List[Dict]) -> List[Dict]:
    if not entries:
        return []

    user_prompt = f"""è¯·å¯¹ä»¥ä¸‹ {len(entries)} æ¡ RSS æ¡ç›®æ‰“åˆ†ï¼š

{json.dumps(compact_for_scoring(entries), ensure_ascii=False, indent=2)}

è¿”å›æ ¼å¼ï¼š
{{
  "scores": [
    {{"link": "...", "score": 8.5, "reason": "..."}},
    ...
  ]
}}"""

    result = llm_client.call_json(SYSTEM_PROMPT_SCORE, user_prompt)
    scores = result.get("scores", [])
    
    # æ’åºå¹¶å– Top N
    scores.sort(key=lambda x: x.get("score", 0), reverse=True)
    top_scores = scores[:TOP_N]

    # è¡¥å……å®Œæ•´ä¿¡æ¯
    link_map = {e["link"]: e for e in entries}
    top_entries = []
    for s in top_scores:
        link = s["link"]
        if link in link_map:
            entry = link_map[link].copy()
            entry["score"] = s["score"]
            entry["score_reason"] = s["reason"]
            top_entries.append(entry)

    logger.info(f"è¯„åˆ†å®Œæˆï¼ŒTop {TOP_N}: {len(top_entries)} æ¡")
    return top_entries

# ==================== æ—¥æŠ¥ç”Ÿæˆé˜¶æ®µ ====================
def generate_daily_report(llm_client: LLMClient, top_entries: List[Dict]) -> Dict:
    today = datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d")
    
    user_prompt = f"""åŸºäºä»¥ä¸‹ Top 3 RSS æ¡ç›®ï¼Œç”Ÿæˆä¸€å¼ "ç»ˆç‰ˆ AI æ—¥æŠ¥å¡ç‰‡"çš„ JSONï¼ˆä¸­æ–‡ï¼‰ï¼Œç»“æ„å¿…é¡»å®Œå…¨ç¬¦åˆä¸‹é¢çš„ JSON å¥‘çº¦ã€‚

ã€Top 3 æ¡ç›®ã€‘
{json.dumps(top_entries, ensure_ascii=False, indent=2)}

ã€JSON å¥‘çº¦ã€‘
{{
  "date": "{today}",
  "theme": "ä»Šæ—¥ä¸»é¢˜ï¼ˆ15å­—ä»¥å†…ï¼‰",
  "items": [
    {{
      "title": "æ¡ç›®æ ‡é¢˜",
      "publish_date": "YYYY-MM-DD",
      "source_type": "æŠ€æœ¯åšå®¢/è®ºæ–‡/...",
      "source_name": "OpenAI/Google/...",
      "erp_relevance": "ğŸ”´ é«˜ / ğŸŸ¡ ä¸­ / ğŸ”µ ä½",
      "summary": "æ ¸å¿ƒæ‘˜è¦ï¼ˆæ˜ç¡®ç‚¹å‡ºæ‰€å±æ–¹å‘ï¼Œå¦‚â€œå±åŸç”ŸAIå®è·µâ€ç­‰ï¼‰",
      "key_facts": "å…³é”®äº‹å®ï¼ˆæ•°æ®æˆ–ç»“è®ºï¼‰",
      "implementation_method": "å®æ–½æ–¹æ³•ï¼ˆæ³¨æ˜æŠ€æœ¯æ ˆï¼‰",
      "exploration_direction": "é¢å‘æˆ‘æ–¹ERPé‡æ„çš„å…·ä½“å»ºè®®",
      "link": "åŸæ–‡é“¾æ¥"
    }}
  ]
}}

ã€ç¡¬çº¦æŸã€‘
- items æ•°ç»„å¿…é¡»åŒ…å«æ‰€æœ‰ 3 æ¡è¾“å…¥å†…å®¹ï¼ˆå¦‚æœä¸è¶³3æ¡åˆ™å…¨éƒ¨åŒ…å«ï¼‰ã€‚
- summary éœ€ç®€ç»ƒï¼Œhighlight ERP relevance.
- implementation_method å¿…é¡»è¯†åˆ«å‡ºå…·ä½“çš„ tool/library/frameworkï¼Œå¦‚æœæ²¡æœ‰åˆ™å†™â€œé€šç”¨å¤§æ¨¡å‹èƒ½åŠ›â€ã€‚
- exploration_direction å¿…é¡»å…·ä½“ã€‚"""

    report = llm_client.call_json(SYSTEM_PROMPT_REPORT, user_prompt)
    return validate_and_fix_report(report)

def validate_and_fix_report(report: Dict) -> Dict:
    """æ ¡éªŒå¹¶ä¿®å¤æ—¥æŠ¥ JSON ç»“æ„"""
    today = datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d")
    
    if "date" not in report: report["date"] = today
    if "theme" not in report: report["theme"] = "AI æŠ€æœ¯åŠ¨æ€"
    if "items" not in report or not isinstance(report["items"], list): report["items"] = []
    
    # ç®€å•çš„å­—æ®µè¡¥å…¨
    for item in report["items"]:
        if "title" not in item: item["title"] = "æœªçŸ¥æ ‡é¢˜"
        if "source_type" not in item: item["source_type"] = "æœªçŸ¥"
        if "erp_relevance" not in item: item["erp_relevance"] = "ğŸ”µ ä½"
        if "summary" not in item: item["summary"] = "æš‚æ— æ‘˜è¦"
    
    logger.info("æ—¥æŠ¥ç»“æ„æ ¡éªŒå®Œæˆ")
    return report

# ==================== é£ä¹¦æ¨é€ ====================
def send_to_feishu(report: Dict):
    if not FEISHU_WEBHOOK_URL:
        logger.warning("æœªé…ç½® FEISHU_WEBHOOK_URLï¼Œè·³è¿‡å‘é€")
        return

    timestamp = str(int(time.time()))
    sign = ""
    if FEISHU_SECRET:
        string_to_sign = f"{timestamp}\n{FEISHU_SECRET}"
        hmac_code = hmac.new(string_to_sign.encode("utf-8"), digestmod=hashlib.sha256).digest()
        sign = base64.b64encode(hmac_code).decode("utf-8")

    # æ„é€ å¡ç‰‡å…ƒç´ 
    elements = [
        {"tag": "div", "text": {"tag": "lark_md", "content": f"**ğŸ“Œ ä»Šæ—¥ä¸»é¢˜ï¼š{report.get('theme')}**"}}
    ]

    items = report.get("items", [])
    for idx, item in enumerate(items, 1):
        elements.append({"tag": "hr"})
        
        # æ„é€ å•æ¡å†…å®¹çš„ markdown
        content_md = f"**æ ‡é¢˜ï¼š{idx}. [{item.get('title')}]({item.get('link')})**\n"
        content_md += f"**å‘å¸ƒæ—¥æœŸï¼š** {item.get('publish_date')} | **ä¿¡æºç±»å‹ï¼š** {item.get('source_type')}ï¼ˆ{item.get('source_name')}ï¼‰ | **ERPç›¸å…³æ€§ï¼š** {item.get('erp_relevance')}\n"
        content_md += f"**æ ¸å¿ƒæ‘˜è¦ï¼š** {item.get('summary')}\n\n"
        content_md += "**æ ¸å¿ƒæ´å¯Ÿï¼š**\n"
        content_md += f"ğŸ”¹ **å…³é”®äº‹å®ï¼š** {item.get('key_facts')}\n"
        content_md += f"ğŸ”¹ **å®æ–½æ–¹æ³•ï¼š** {item.get('implementation_method')}\n"
        content_md += f"ğŸ”¹ **æ¢ç´¢æ–¹å‘ï¼š** {item.get('exploration_direction')}\n\n"
        content_md += f"**åŸæ–‡é“¾æ¥ï¼š** {item.get('link')}"

        elements.append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": content_md
            }
        })

    card = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"ğŸ“° AI æ—¥æŠ¥ | {report.get('date')}"},
            "template": "blue"
        },
        "elements": elements
    }

    payload = {
        "timestamp": timestamp,
        "sign": sign,
        "msg_type": "interactive",
        "card": card
    }

    for attempt in range(3):
        try:
            resp = requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=10)
            resp.raise_for_status()
            res_json = resp.json()
            if res_json.get("code") == 0:
                logger.info("é£ä¹¦æ¨é€æˆåŠŸ")
                return
            logger.warning(f"é£ä¹¦æ¨é€å¤±è´¥: {res_json}")
        except Exception as e:
            logger.warning(f"é£ä¹¦æ¨é€å°è¯• {attempt+1} å¤±è´¥: {e}")
            if attempt < 2:
                time.sleep(2 ** attempt)
    logger.error("é£ä¹¦æ¨é€æœ€ç»ˆå¤±è´¥")

# ==================== ä¸»æµç¨‹ ====================
def main():
    logger.info("å¼€å§‹æ‰§è¡Œ AI æ—¥æŠ¥ä»»åŠ¡")
    
    # 0. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    llm_client = get_llm_client()

    # 1. æŠ“å– RSS
    candidates = fetch_rss_entries()
    if not candidates:
        logger.info("æ— æ–°å†…å®¹ï¼Œé€€å‡º")
        return

    # 2. è¯„åˆ†
    top_entries = score_entries(llm_client, candidates)
    if not top_entries:
        logger.info("æ— é«˜åˆ†å†…å®¹ï¼Œé€€å‡º")
        return

    # 3. ç”Ÿæˆæ—¥æŠ¥
    report = generate_daily_report(llm_client, top_entries)
    logger.info("æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")
    print(json.dumps(report, ensure_ascii=False, indent=2))

    # 4. å‘é€é£ä¹¦
    send_to_feishu(report)

    # 5. æ›´æ–°å»é‡æ–‡ä»¶
    sent_hashes = load_sent_hashes()
    new_hashes = {e["hash"] for e in top_entries}
    sent_hashes.update(new_hashes)
    save_sent_hashes(sent_hashes)
    logger.info(f"å·²æ›´æ–°å»é‡æ–‡ä»¶ï¼Œæ–°å¢ {len(new_hashes)} æ¡")

if __name__ == "__main__":
    main()
